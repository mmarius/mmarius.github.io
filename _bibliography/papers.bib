---
---

@inproceedings{alabi-etal-2022-adapting,
    abbr = {COLING 2022},
    title = "Adapting Pre-trained Language Models to {A}frican Languages via Multilingual Adaptive Fine-Tuning",
    author = "Alabi, Jesujoba O.  and
      Adelani, David Ifeoluwa  and
      Mosbach, Marius  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.382",
    pages = "4336--4349",
    abstract = "Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) {---} fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50{\%}. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.",
}


@article{zhang2022mcse,
    abbr = {NAACL 2022},
    title={MCSE: Multimodal Contrastive Learning of Sentence Embeddings},
    author={Zhang, Miaoran and Mosbach, Marius and Adelani, David Ifeoluwa and Hedderich, Michael A. and Klakow, Dietrich},
    journal={NAACL 2022},
    year={2022},
    abstract= "Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman's correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.",
    html={https://arxiv.org/abs/2204.10931},
}


@article{zouhar2021knowledge,
    abbr = {SpaNLP 2022},
    title={Knowledge Base Index Compression via Dimensionality and Precision Reduction},
    author={Zouhar, Vilém and Mosbach, Marius and Klakow, Dietrich},
    journal={SpaNLP workshop @ ACL 2022},
    year={2022},
    abstract= "Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. Retrieval is typically performed over a large textual knowledge base (KB) which requires significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction. Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable. All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction. Finally, we show that it is possible to combine PCA with using 1bit per dimension. Overall we achieve (1) 100× compression with 75%, and (2) 24× compression with 92% original retrieval performance.",
    html={https://arxiv.org/abs/2204.02906},
}

@article{zouhar2021artefact,
    abbr = {AKBC 2021},
    title={Artefact Retrieval: Overview of NLP Models with Knowledge Base Access},
    author={Zouhar, Vilém and Mosbach, Marius and Biswas, Debanjali and Klakow, Dietrich},
    journal={CSKB workshop @ AKBC},
    year={2021},
    abstract= "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems.
In this paper, we systematically describe the typology of *artefacts* (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are *fused* into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.",
    html={https://openreview.net/forum?id=9_oCNR6R9l2},
    code={https://github.com/uds-lsv},
}

@article{abdullah2021do,
    abbr = {Interspeech 2021},
    title={Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study},
    author={Abdullah, Badr M. and Mosbach, Marius and Zaitova, Iuliia and Möbius, Bernd and Klakow, Dietrich},
    journal={Interspeech},
    year={2021},
    abstract= "Several variants of deep neural networks have been successfully employed for building parametric models that project variable-duration spoken word segments onto fixed-size vector representations, or acoustic word embeddings (AWEs). However, it remains unclear to what degree we can rely on the distance in the emerging AWE space as an estimate of word-form similarity. In this paper, we ask: does the distance in the acoustic embedding space correlate with phonological dissimilarity? To answer this question, we empirically investigate the performance of supervised approaches for AWEs with different neural architectures and learning objectives. We train AWE models in controlled settings for two languages (German and Czech) and evaluate the embeddings on two tasks: word discrimination and phonological similarity. Our experiments show that (1) the distance in the embedding space in the best cases only moderately correlates with phonological distance, and (2) improving the performance on the word discrimination task does not necessarily yield models that better reflect word phonological similarity. Our findings highlight the necessity to rethink the current intrinsic evaluations for AWEs.",
    html={https://arxiv.org/abs/2106.08686},
    code={https://github.com/uds-lsv},
}

@article{mosbach2020closer,
    abbr = {COLING 2020},
    title={A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English},
    author={Mosbach, Marius and Degaetano-Ortlieb, Stefania and Krielke, Marie-Pauline and Abdullah, Badr M. and Klakow, Dietrich},
    journal={COLING},
    year={2020},
    abstract= "Transformer-based language models achieve high performance on various tasks, but we still lack understanding of the kind of linguistic knowledge they learn and rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks. We focus on relative clauses (in American English) as a complex phenomenon needing contextual information and antecedent identification to be resolved. Based on a naturalistic dataset, probing shows that all three models indeed capture linguistic knowledge about grammaticality, achieving high performance. Evaluation on diagnostic cases and masked prediction tasks considering fine-grained linguistic knowledge, however, shows pronounced model-specific weaknesses especially on semantic knowledge, strongly impacting models' performance. Our results highlight the importance of (a)model comparison in evaluation task and (b) building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.",
    html={https://arxiv.org/abs/2011.00960},
    code={https://github.com/uds-lsv},
}

@article{mosbach2020stability,
    abbr = {EMNLP 2020},
    title={On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers},
    author={Mosbach, Marius and Khokhlova, Anna and Hedderich, Michael A. and Klakow, Dietrich},
    journal={Findings of EMNLP and BlackboxNLP},
    year={2020},
    abstract= "Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.",
    html={https://arxiv.org/abs/2010.02616},
    code={https://github.com/uds-lsv},
}

@article{mosbach2020stability,
    abbr = {ICLR 2020},
    title={On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
    author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
    journal={ICLR},
    year={2020},
    abstract= "Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and a small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on three commonly used datasets from the GLUE benchmark and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than previously proposed approaches.",
    html={https://arxiv.org/abs/2006.04884},
    code={https://github.com/uds-lsv/bert-stable-fine-tuning},
}

@article{mogadala2020sparse,
    abbr = {ICML 2020},
    title={Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation},
    author={Mogadala, Aditya and Mosbach, Marius and Klakow, Dietrich},
    journal={ICML Workshop on Bridge Between Perception and Reasoning:
Graph Neural Networks & Beyond},
    year={2020},
    abstract= "Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation (e.g., image or video captioning) as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture. To be specific, we introduce Sparse Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.",
    html={https://arxiv.org/abs/2007.06077},
}


@inproceedings{bizzoni-etal-2019-steps,
    abbr={NoDaLiDa 2019},
    title = "Some steps towards the generation of diachronic {W}ord{N}ets",
    author = "Bizzoni, Yuri  and
      Mosbach, Marius  and
      Klakow, Dietrich  and
      Degaetano-Ortlieb, Stefania",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://www.aclweb.org/anthology/W19-6106",
    pages = "55--64",
    abstract = "We apply hyperbolic embeddings to trace the dynamics of change of conceptual-semantic relationships in a large diachronic scientific corpus (200 years). Our focus is on emerging scientific fields and the increasingly specialized terminology establishing around them. Reproducing high-quality hierarchical structures such as WordNet on a diachronic scale is a very difficult task. Hyperbolic embeddings can map partial graphs into low dimensional, continuous hierarchical spaces, making more explicit the latent structure of the input. We show that starting from simple lists of word pairs (rather than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields.",
    html={https://www.aclweb.org/anthology/W19-6106/},
}

@inproceedings{mosbach-etal-2019-incom,
    abbr={RANLP 2019},
    title = "incom.py - A Toolbox for Calculating Linguistic Distances and Asymmetries between Related Languages",
    author = "Mosbach, Marius  and
      Stenger, Irina  and
      Avgustinova, Tania  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://www.aclweb.org/anthology/R19-1094",
    doi = "10.26615/978-954-452-056-4_094",
    pages = "810--818",
    abstract = "Languages may be differently distant from each other and their mutual intelligibility may be asymmetric. In this paper we introduce incom.py, a toolbox for calculating linguistic distances and asymmetries between related languages. incom.py allows linguist experts to quickly and easily perform statistical analyses and compare those with experimental results. We demonstrate the efficacy of incom.py in an incomprehension experiment on two Slavic languages: Bulgarian and Russian. Using incom.py we were able to validate three methods to measure linguistic distances and asymmetries: Levenshtein distance, word adaptation surprisal, and conditional entropy as predictors of success in a reading intercomprehension experiment.",
    html={https://www.aclweb.org/anthology/R19-1094/},
    code={https://github.com/uds-lsv/incompy},
}

@article{mosbach2018logit,
    abbr = {NeurIPS 2018},
    title = {Logit pairing methods can fool gradient-based attacks},
    author = {Mosbach, Marius and Andriushchenko, Maksym and Trost, Thomas and Hein, Matthias and Klakow, Dietrich},
    journal = {NeurIPS Workshop on Security in Machine Learning},
    year = {2018},
    abstract = "Recently, Kannan et al. [2018] proposed several logit regularization methods to improve the adversarial robustness of classifiers. We show that the computationally fast methods they propose - Clean Logit Pairing (CLP) and Logit Squeezing (LSQ) - just make the gradient-based optimization problem of crafting adversarial examples harder without providing actual robustness. We find that Adversarial Logit Pairing (ALP) may indeed provide robustness against adversarial examples, especially when combined with adversarial training, and we examine it in a variety of settings. However, the increase in adversarial accuracy is much smaller than previously claimed. Finally, our results suggest that the evaluation against an iterative PGD attack relies heavily on the parameters used and may result in false conclusions regarding robustness of a model.",
    html={https://arxiv.org/abs/1810.12042},
    code={https://github.com/uds-lsv/evaluating-logit-pairing-methods},
}
