---
---

@article{mosbach2020closer,
    abbr = {COLING 2020},
    title={A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English},
    author={Mosbach, Marius and Degaetano-Ortlieb, Stefania and Krielke, Marie-Pauline and Abdullah, Badr M. and Klakow, Dietrich},
    journal={To appear at COLING 2020},
    year={2020},
    abstract= "Transformer-based language models achieve high performance on various tasks, but we still lack understanding of the kind of linguistic knowledge they learn and rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks. We focus on relative clauses (in American English) as a complex phenomenon needing contextual information and antecedent identification to be resolved. Based on a naturalistic dataset, probing shows that all three models indeed capture linguistic knowledge about grammaticality, achieving high performance. Evaluation on diagnostic cases and masked prediction tasks considering fine-grained linguistic knowledge, however, shows pronounced model-specific weaknesses especially on semantic knowledge, strongly impacting models' performance. Our results highlight the importance of (a)model comparison in evaluation task and (b) building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.",
    html={https://arxiv.org/abs/2011.00960},
    code={https://github.com/uds-lsv/rc-probing},
}

@article{mosbach2020stability,
    abbr = {EMNLP 2020},
    title={On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers},
    author={Mosbach, Marius and Khokhlova, Anna and Hedderich, Michael A. and Klakow, Dietrich},
    journal={To appear at Findings of EMNLP 2020 and BlackboxNLP},
    year={2020},
    abstract= "Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.",
    html={https://arxiv.org/abs/2010.02616},
    code={https://github.com/uds-lsv/probing-and-finetuning},
}

@article{mosbach2020stability,
    abbr = {Preprint},
    title={On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
    author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
    journal={arXiv preprint arXiv:2006.04884},
    year={2020},
    abstract= "Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and a small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on three commonly used datasets from the GLUE benchmark and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than previously proposed approaches.",
    html={https://arxiv.org/abs/2007.06077},
    code={https://github.com/uds-lsv/bert-stable-fine-tuning},
}

@article{mogadala2020sparse,
    abbr = {ICML 2020},
    title={Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation},
    author={Mogadala, Aditya and Mosbach, Marius and Klakow, Dietrich},
    journal={ICML Workshop on Bridge Between Perception and Reasoning:
Graph Neural Networks & Beyond},
    year={2020},
    abstract= "Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation (e.g., image or video captioning) as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture. To be specific, we introduce Sparse Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.",
    html={https://arxiv.org/abs/2007.06077},
}


@inproceedings{bizzoni-etal-2019-steps,
    abbr={NoDaLiDa 2019},
    title = "Some steps towards the generation of diachronic {W}ord{N}ets",
    author = "Bizzoni, Yuri  and
      Mosbach, Marius  and
      Klakow, Dietrich  and
      Degaetano-Ortlieb, Stefania",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://www.aclweb.org/anthology/W19-6106",
    pages = "55--64",
    abstract = "We apply hyperbolic embeddings to trace the dynamics of change of conceptual-semantic relationships in a large diachronic scientific corpus (200 years). Our focus is on emerging scientific fields and the increasingly specialized terminology establishing around them. Reproducing high-quality hierarchical structures such as WordNet on a diachronic scale is a very difficult task. Hyperbolic embeddings can map partial graphs into low dimensional, continuous hierarchical spaces, making more explicit the latent structure of the input. We show that starting from simple lists of word pairs (rather than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields.",
    html={https://www.aclweb.org/anthology/W19-6106/},
}

@inproceedings{mosbach-etal-2019-incom,
    abbr={RANLP 2019},
    title = "incom.py - A Toolbox for Calculating Linguistic Distances and Asymmetries between Related Languages",
    author = "Mosbach, Marius  and
      Stenger, Irina  and
      Avgustinova, Tania  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://www.aclweb.org/anthology/R19-1094",
    doi = "10.26615/978-954-452-056-4_094",
    pages = "810--818",
    abstract = "Languages may be differently distant from each other and their mutual intelligibility may be asymmetric. In this paper we introduce incom.py, a toolbox for calculating linguistic distances and asymmetries between related languages. incom.py allows linguist experts to quickly and easily perform statistical analyses and compare those with experimental results. We demonstrate the efficacy of incom.py in an incomprehension experiment on two Slavic languages: Bulgarian and Russian. Using incom.py we were able to validate three methods to measure linguistic distances and asymmetries: Levenshtein distance, word adaptation surprisal, and conditional entropy as predictors of success in a reading intercomprehension experiment.",
    html={https://www.aclweb.org/anthology/R19-1094/},
    code={https://github.com/uds-lsv/incompy},
}

@article{mosbach2018logit,
    abbr = {NeurIPS 2018},
    title = {Logit pairing methods can fool gradient-based attacks},
    author = {Mosbach, Marius and Andriushchenko, Maksym and Trost, Thomas and Hein, Matthias and Klakow, Dietrich},
    journal = {NeurIPS Workshop on Security in Machine Learning},
    year = {2018},
    abstract = "Recently, Kannan et al. [2018] proposed several logit regularization methods to improve the adversarial robustness of classifiers. We show that the computationally fast methods they propose - Clean Logit Pairing (CLP) and Logit Squeezing (LSQ) - just make the gradient-based optimization problem of crafting adversarial examples harder without providing actual robustness. We find that Adversarial Logit Pairing (ALP) may indeed provide robustness against adversarial examples, especially when combined with adversarial training, and we examine it in a variety of settings. However, the increase in adversarial accuracy is much smaller than previously claimed. Finally, our results suggest that the evaluation against an iterative PGD attack relies heavily on the parameters used and may result in false conclusions regarding robustness of a model.",
    html={https://arxiv.org/abs/1810.12042},
    code={https://github.com/uds-lsv/evaluating-logit-pairing-methods},
}
