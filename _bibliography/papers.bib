@article{mosbach2024from,
    abbr = {EMNLP 2024},
    title={From Insights to Actions: The Impact of Interpretability and Analysis Research on NLP},
    author={Mosbach, Marius and Gautam, Vagrant and Vergara-Browne, Tomás and Klakow, Dietrich and Geva, Mor},
    journal={EMNLP},
    year={2024},
    abstract= "Interpretability and analysis (IA) research is a growing subfield within NLP with the goal of developing a deeper understanding of the behavior or inner workings of NLP systems and methods. Despite growing interest in the subfield, a commonly voiced criticism is that it lacks actionable insights and therefore has little impact on NLP. In this paper, we seek to quantify the impact of IA research on the broader field of NLP. We approach this with a mixed-methods analysis of: (1) a citation graph of 185K+ papers built from all papers published at ACL and EMNLP conferences from 2018 to 2023, and (2) a survey of 138 members of the NLP community. Our quantitative results show that IA work is well-cited outside of IA, and central in the NLP citation graph. Through qualitative analysis of survey responses and manual annotation of 556 papers, we find that NLP researchers build on findings from IA work and perceive it is important for progress in NLP, multiple subfields, and rely on its findings and terminology for their own work. Many novel methods are proposed based on IA findings and highly influenced by them, but highly influential non-IA work cites IA findings without being driven by them. We end by summarizing what is missing in IA work today and provide a call to action, to pave the way for a more impactful future of IA research.",
    html={https://arxiv.org/abs/2406.12618},
    code={https://github.com/mmarius/interpretability-impact},
    conference = "EMNLP 2024",
}

@article{behnamghader2024llm2vec,
    abbr = {COLM 2024},
    title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders},
    author={BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
    journal={COLM},
    year={2024},
    abstract= "Large decoder-only language models (LLMs) are the state-of-the-art models on most of today's NLP tasks and benchmarks. Yet, the community is only slowly adopting these models for text embedding tasks, which require rich contextualized representations. In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks. We outperform encoder-only models by a large margin on word-level tasks and reach a new unsupervised state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB). Moreover, when combining LLM2Vec with supervised contrastive learning, we achieve state-of-the-art performance on MTEB among models that train only on publicly available data. Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders in a parameter-efficient manner without the need for expensive adaptation or synthetic GPT-4 generated data.",
    html={https://arxiv.org/abs/2404.05961},
    code={https://mcgill-nlp.github.io/llm2vec/},
    conference = "COLM 2024",
}

@article{garcia2024what,
    abbr = {Insights 2024},
    title={What explains the success of cross-modal fine-tuning with ORCA?},
    author={García-de-Herreros, Paloma and Gautam, Vagrant and Slusallek, Philipp and Klakow, Dietrich and Mosbach, Marius},
    journal={Workshop on Insights from Negative Results in NLP @ ACL},
    year={2024},
    abstract= "ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data. The technique consists primarily of training an embedder and fine-tuning the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.",
    html={https://arxiv.org/abs/2403.13537},
}

@article{alabi2024hidden,
    abbr = {ACL 2024},
    title={The Hidden Space of Transformer Language Adapters},
    author={Alabi, Jesujoba O. and Mosbach, Marius and Eyal, Matan and Klakow, Dietrich and Geva, Mor},
    journal={ACL},
    year={2024},
    abstract= "We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.",
    html={https://arxiv.org/abs/2402.13137},
    conference = "ACL 2024",
}

@article{zhang2024impact,
    abbr = {ACL 2024},
    title={The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis},
    author={Zhang, Miaoran and Gautam, Vagrant and Wang, Mingyang and Alabi, Jesujoba O. and Shen, Xiaoyu and Klakow, Dietrich and Mosbach, Marius},
    journal={ACL},
    year={2024},
    abstract= "In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.",
    html={https://arxiv.org/abs/2402.12976},
    conference = "ACL 2024",
}

@inproceedings{steuer-etal-2023-large,
    abbr = {BabyLM 2023},
    title = "Large GPT-like Models are Bad Babies: A Closer Look at the Relationship between Linguistic Competence and Psycholinguistic Measures",
    author = "Steuer, Julius  and
      Mosbach, Marius  and
      Klakow, Dietrich",
    month = dec,
    year = "2023",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2311.04547",
    abstract = "Research on the cognitive plausibility of language models (LMs) has so far mostly concentrated on modelling psycholinguistic response variables such as reading times, gaze durations and N400/P600 EEG signals, while mostly leaving out the dimension of what Mahowald et al. (2023) described as formal and functional linguistic competence, and developmental plausibility. We address this gap by training a series of GPT-like language models of different sizes on the strict version of the BabyLM pretraining corpus, evaluating on the challenge tasks (BLiMP, GLUE, MSGS) and an additional reading time prediction task. We find a positive correlation between LM size and performance on all three challenge tasks, with different preferences for model width and depth in each of the tasks. In contrast, a negative correlation was found between LM size and reading time fit of linear mixed-effects models using LM surprisal as a predictor, with the second-smallest LM achieving the largest log-likelihood reduction over a baseline model without surprisal. This suggests that modelling processing effort and linguistic competence may require an approach different from training GPT-like LMs on a developmentally plausible corpus.",
    award = "Best paper",
    html={https://arxiv.org/abs/2311.04547},
}

@inproceedings{mosbach-etal-2023-shot,
    abbr = {ACL 2023},
    title = "Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation",
    author = "Mosbach, Marius  and
      Pimentel, Tiago  and
      Ravfogel, Shauli  and
      Klakow, Dietrich  and
      Elazar, Yanai",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.779",
    pages = "12284--12314",
    abstract = "Few-shot fine-tuning and in-context learning are two alternative strategies for task adaptation of pre-trained language models. Recently, in-context learning has gained popularity over fine-tuning due to its simplicity and improved out-of-domain generalization, and because extensive evidence shows that fine-tuned models pick up on spurious correlations.Unfortunately, previous comparisons of the two approaches were done using models of different sizes. This raises the question of whether the observed weaker out-of-domain generalization of fine-tuned models is an inherent property of fine-tuning or a limitation of the experimental setup. In this paper, we compare the generalization of few-shot fine-tuning and in-context learning to challenge datasets, while controlling for the models used, the number of examples, and the number of parameters, ranging from 125M to 30B. Our results show that fine-tuned language models can in fact generalize well out-of-domain. We find that both approaches generalize similarly; they exhibit large variation and depend on properties such as model size and the number of examples, highlighting that robust task adaptation remains a challenge.",
    html={https://aclanthology.org/2023.findings-acl.779},
}

@inproceedings{zhu-etal-2023-weaker,
    abbr = {ACL 2023},
    title = "Weaker Than You Think: A Critical Look at Weakly Supervised Learning",
    author = "Zhu, Dawei  and
      Shen, Xiaoyu  and
      Mosbach, Marius  and
      Stephan, Andreas  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.796",
    pages = "14229--14253",
    abstract = "Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research.",
    award = "Best paper",
    html={https://aclanthology.org/2023.acl-long.796},
}

@inproceedings{alabi-etal-2022-adapting,
    abbr = {COLING 2022},
    title = "Adapting Pre-trained Language Models to {A}frican Languages via Multilingual Adaptive Fine-Tuning",
    author = "Alabi, Jesujoba O.  and
      Adelani, David Ifeoluwa  and
      Mosbach, Marius  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2022.coling-1.382",
    pages = "4336--4349",
    abstract = "Multilingual pre-trained language models (PLMs) have demonstrated impressive performance on several downstream tasks for both high-resourced and low-resourced languages. However, there is still a large performance drop for languages unseen during pre-training, especially African languages. One of the most effective approaches to adapt to a new language is language adaptive fine-tuning (LAFT) {---} fine-tuning a multilingual PLM on monolingual texts of a language using the pre-training objective. However, adapting to target language individually takes large disk space and limits the cross-lingual transfer abilities of the resulting models because they have been specialized for a single language. In this paper, we perform multilingual adaptive fine-tuning on 17 most-resourced African languages and three other high-resource languages widely spoken on the African continent to encourage cross-lingual transfer learning. To further specialize the multilingual PLM, we removed vocabulary tokens from the embedding layer that corresponds to non-African writing scripts before MAFT, thus reducing the model size by around 50{\%}. Our evaluation on two multilingual PLMs (AfriBERTa and XLM-R) and three NLP tasks (NER, news topic classification, and sentiment classification) shows that our approach is competitive to applying LAFT on individual languages while requiring significantly less disk space. Additionally, we show that our adapted PLM also improves the zero-shot cross-lingual transfer abilities of parameter efficient fine-tuning methods.",
    award = "Best paper",
    html={https://aclanthology.org/2022.coling-1.382},
}


@article{zhang2022mcse,
    abbr = {NAACL 2022},
    title={MCSE: Multimodal Contrastive Learning of Sentence Embeddings},
    author={Zhang, Miaoran and Mosbach, Marius and Adelani, David Ifeoluwa and Hedderich, Michael A. and Klakow, Dietrich},
    journal={NAACL 2022},
    year={2022},
    abstract= "Learning semantically meaningful sentence embeddings is an open problem in natural language processing. In this work, we propose a sentence embedding learning approach that exploits both visual and textual information via a multimodal contrastive objective. Through experiments on a variety of semantic textual similarity tasks, we demonstrate that our approach consistently improves the performance across various datasets and pre-trained encoders. In particular, combining a small amount of multimodal data with a large text-only corpus, we improve the state-of-the-art average Spearman's correlation by 1.7%. By analyzing the properties of the textual embedding space, we show that our model excels in aligning semantically similar sentences, providing an explanation for its improved performance.",
    html={https://aclanthology.org/2022.naacl-main.436/},
}


@article{zouhar2021knowledge,
    abbr = {SpaNLP 2022},
    title={Knowledge Base Index Compression via Dimensionality and Precision Reduction},
    author={Zouhar, Vilém and Mosbach, Marius and Klakow, Dietrich},
    journal={SpaNLP workshop @ ACL 2022},
    year={2022},
    abstract= "Recently neural network based approaches to knowledge-intensive NLP tasks, such as question answering, started to rely heavily on the combination of neural retrievers and readers. Retrieval is typically performed over a large textual knowledge base (KB) which requires significant memory and compute resources, especially when scaled up. On HotpotQA we systematically investigate reducing the size of the KB index by means of dimensionality (sparse random projections, PCA, autoencoders) and numerical precision reduction. Our results show that PCA is an easy solution that requires very little data and is only slightly worse than autoencoders, which are less stable. All methods are sensitive to pre- and post-processing and data should always be centered and normalized both before and after dimension reduction. Finally, we show that it is possible to combine PCA with using 1bit per dimension. Overall we achieve (1) 100× compression with 75%, and (2) 24× compression with 92% original retrieval performance.",
    html={https://arxiv.org/abs/2204.02906},
}

@article{zouhar2021artefact,
    abbr = {AKBC 2021},
    title={Artefact Retrieval: Overview of NLP Models with Knowledge Base Access},
    author={Zouhar, Vilém and Mosbach, Marius and Biswas, Debanjali and Klakow, Dietrich},
    journal={CSKB workshop @ AKBC},
    year={2021},
    abstract= "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems.
In this paper, we systematically describe the typology of *artefacts* (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are *fused* into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.",
    html={https://openreview.net/forum?id=9_oCNR6R9l2},
    code={https://github.com/uds-lsv},
}

@article{abdullah2021do,
    abbr = {Interspeech 2021},
    title={Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study},
    author={Abdullah, Badr M. and Mosbach, Marius and Zaitova, Iuliia and Möbius, Bernd and Klakow, Dietrich},
    journal={Interspeech},
    year={2021},
    abstract= "Several variants of deep neural networks have been successfully employed for building parametric models that project variable-duration spoken word segments onto fixed-size vector representations, or acoustic word embeddings (AWEs). However, it remains unclear to what degree we can rely on the distance in the emerging AWE space as an estimate of word-form similarity. In this paper, we ask: does the distance in the acoustic embedding space correlate with phonological dissimilarity? To answer this question, we empirically investigate the performance of supervised approaches for AWEs with different neural architectures and learning objectives. We train AWE models in controlled settings for two languages (German and Czech) and evaluate the embeddings on two tasks: word discrimination and phonological similarity. Our experiments show that (1) the distance in the embedding space in the best cases only moderately correlates with phonological distance, and (2) improving the performance on the word discrimination task does not necessarily yield models that better reflect word phonological similarity. Our findings highlight the necessity to rethink the current intrinsic evaluations for AWEs.",
    html={https://arxiv.org/abs/2106.08686},
    code={https://github.com/uds-lsv},
}

@article{mosbach2021stability,
    abbr = {ICLR 2021},
    title={On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
    author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
    journal={ICLR},
    year={2021},
    abstract= "Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and a small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on three commonly used datasets from the GLUE benchmark and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than previously proposed approaches.",
    html={https://arxiv.org/abs/2006.04884},
    code={https://github.com/uds-lsv/bert-stable-fine-tuning},
}

@article{mosbach2020closer,
    abbr = {COLING 2020},
    title={A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English},
    author={Mosbach, Marius and Degaetano-Ortlieb, Stefania and Krielke, Marie-Pauline and Abdullah, Badr M. and Klakow, Dietrich},
    journal={COLING},
    year={2020},
    abstract= "Transformer-based language models achieve high performance on various tasks, but we still lack understanding of the kind of linguistic knowledge they learn and rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks. We focus on relative clauses (in American English) as a complex phenomenon needing contextual information and antecedent identification to be resolved. Based on a naturalistic dataset, probing shows that all three models indeed capture linguistic knowledge about grammaticality, achieving high performance. Evaluation on diagnostic cases and masked prediction tasks considering fine-grained linguistic knowledge, however, shows pronounced model-specific weaknesses especially on semantic knowledge, strongly impacting models' performance. Our results highlight the importance of (a)model comparison in evaluation task and (b) building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.",
    html={https://arxiv.org/abs/2011.00960},
    code={https://github.com/uds-lsv},
}

@article{mosbach2020interplay,
    abbr = {EMNLP 2020},
    title={On the Interplay Between Fine-tuning and Sentence-level Probing for Linguistic Knowledge in Pre-trained Transformers},
    author={Mosbach, Marius and Khokhlova, Anna and Hedderich, Michael A. and Klakow, Dietrich},
    journal={Findings of EMNLP and BlackboxNLP},
    year={2020},
    abstract= "Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.",
    html={https://arxiv.org/abs/2010.02616},
    code={https://github.com/uds-lsv},
}

@article{mogadala2020sparse,
    abbr = {ICML 2020},
    title={Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation},
    author={Mogadala, Aditya and Mosbach, Marius and Klakow, Dietrich},
    journal={ICML Workshop on Bridge Between Perception and Reasoning:
Graph Neural Networks & Beyond},
    year={2020},
    abstract= "Generating longer textual sequences when conditioned on the visual information is an interesting problem to explore. The challenge here proliferate over the standard vision conditioned sentence-level generation (e.g., image or video captioning) as it requires to produce a brief and coherent story describing the visual content. In this paper, we mask this Vision-to-Sequence as Graph-to-Sequence learning problem and approach it with the Transformer architecture. To be specific, we introduce Sparse Graph-to-Sequence Transformer (SGST) for encoding the graph and decoding a sequence. The encoder aims to directly encode graph-level semantics, while the decoder is used to generate longer sequences. Experiments conducted with the benchmark image paragraph dataset show that our proposed achieve 13.3% improvement on the CIDEr evaluation measure when comparing to the previous state-of-the-art approach.",
    html={https://arxiv.org/abs/2007.06077},
}


@inproceedings{bizzoni-etal-2019-steps,
    abbr={NoDaLiDa 2019},
    title = "Some steps towards the generation of diachronic {W}ord{N}ets",
    author = "Bizzoni, Yuri  and
      Mosbach, Marius  and
      Klakow, Dietrich  and
      Degaetano-Ortlieb, Stefania",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://www.aclweb.org/anthology/W19-6106",
    pages = "55--64",
    abstract = "We apply hyperbolic embeddings to trace the dynamics of change of conceptual-semantic relationships in a large diachronic scientific corpus (200 years). Our focus is on emerging scientific fields and the increasingly specialized terminology establishing around them. Reproducing high-quality hierarchical structures such as WordNet on a diachronic scale is a very difficult task. Hyperbolic embeddings can map partial graphs into low dimensional, continuous hierarchical spaces, making more explicit the latent structure of the input. We show that starting from simple lists of word pairs (rather than a list of entities with directional links) it is possible to build diachronic hierarchical semantic spaces which allow us to model a process towards specialization for selected scientific fields.",
    html={https://www.aclweb.org/anthology/W19-6106/},
}

@inproceedings{mosbach-etal-2019-incom,
    abbr={RANLP 2019},
    title = "incom.py - A Toolbox for Calculating Linguistic Distances and Asymmetries between Related Languages",
    author = "Mosbach, Marius  and
      Stenger, Irina  and
      Avgustinova, Tania  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)",
    month = sep,
    year = "2019",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://www.aclweb.org/anthology/R19-1094",
    doi = "10.26615/978-954-452-056-4_094",
    pages = "810--818",
    abstract = "Languages may be differently distant from each other and their mutual intelligibility may be asymmetric. In this paper we introduce incom.py, a toolbox for calculating linguistic distances and asymmetries between related languages. incom.py allows linguist experts to quickly and easily perform statistical analyses and compare those with experimental results. We demonstrate the efficacy of incom.py in an incomprehension experiment on two Slavic languages: Bulgarian and Russian. Using incom.py we were able to validate three methods to measure linguistic distances and asymmetries: Levenshtein distance, word adaptation surprisal, and conditional entropy as predictors of success in a reading intercomprehension experiment.",
    html={https://www.aclweb.org/anthology/R19-1094/},
    code={https://github.com/uds-lsv/incompy},
}

@article{mosbach2018logit,
    abbr = {NeurIPS 2018},
    title = {Logit pairing methods can fool gradient-based attacks},
    author = {Mosbach, Marius and Andriushchenko, Maksym and Trost, Thomas and Hein, Matthias and Klakow, Dietrich},
    journal = {NeurIPS Workshop on Security in Machine Learning},
    year = {2018},
    abstract = "Recently, Kannan et al. [2018] proposed several logit regularization methods to improve the adversarial robustness of classifiers. We show that the computationally fast methods they propose - Clean Logit Pairing (CLP) and Logit Squeezing (LSQ) - just make the gradient-based optimization problem of crafting adversarial examples harder without providing actual robustness. We find that Adversarial Logit Pairing (ALP) may indeed provide robustness against adversarial examples, especially when combined with adversarial training, and we examine it in a variety of settings. However, the increase in adversarial accuracy is much smaller than previously claimed. Finally, our results suggest that the evaluation against an iterative PGD attack relies heavily on the parameters used and may result in false conclusions regarding robustness of a model.",
    html={https://arxiv.org/abs/1810.12042},
    code={https://github.com/uds-lsv/evaluating-logit-pairing-methods},
}
